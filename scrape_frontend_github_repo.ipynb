{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook is for scrape the (open and closed) issues and pull-request, and 'used by' data.\n",
    "Ialso mirror the dataframe, so the number of pairs will be 2 times higher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  \n",
    "import requests  \n",
    "import time  \n",
    "from datetime import datetime  \n",
    "   \n",
    "# Function to fetch GitHub metrics  \n",
    "def get_github_metrics(repo_url, headers):  \n",
    "    # Extract the owner and repo name from the URL  \n",
    "    parts = repo_url.split('/')  \n",
    "    owner = parts[-2]  \n",
    "    repo = parts[-1]  \n",
    "      \n",
    "    # GitHub API URLs  \n",
    "    issues_open_url = f'https://api.github.com/search/issues?q=repo:{owner}/{repo}+type:issue+state:open'  \n",
    "    issues_closed_url = f'https://api.github.com/search/issues?q=repo:{owner}/{repo}+type:issue+state:closed'  \n",
    "    pulls_open_url = f'https://api.github.com/search/issues?q=repo:{owner}/{repo}+type:pr+state:open'  \n",
    "    pulls_closed_url = f'https://api.github.com/search/issues?q=repo:{owner}/{repo}+type:pr+state:closed'  \n",
    "    commits_url = f'https://api.github.com/repos/{owner}/{repo}/commits'  \n",
    "      \n",
    "    # Fetch open issues count  \n",
    "    issues_open_response = requests.get(issues_open_url, headers=headers)  \n",
    "    if issues_open_response.status_code == 200:  \n",
    "        issues_open_data = issues_open_response.json()  \n",
    "        open_issues_count = issues_open_data.get('total_count', 0)  \n",
    "    else:  \n",
    "        print(f\"Failed to fetch open issues for {repo_url}: {issues_open_response.status_code}\")  \n",
    "        open_issues_count = 0  \n",
    "      \n",
    "    # Fetch closed issues count  \n",
    "    issues_closed_response = requests.get(issues_closed_url, headers=headers)  \n",
    "    if issues_closed_response.status_code == 200:  \n",
    "        issues_closed_data = issues_closed_response.json()  \n",
    "        closed_issues_count = issues_closed_data.get('total_count', 0)  \n",
    "    else:  \n",
    "        print(f\"Failed to fetch closed issues for {repo_url}: {issues_closed_response.status_code}\")  \n",
    "        closed_issues_count = 0  \n",
    "      \n",
    "    # Fetch open pull requests count  \n",
    "    pulls_open_response = requests.get(pulls_open_url, headers=headers)  \n",
    "    if pulls_open_response.status_code == 200:  \n",
    "        pulls_open_data = pulls_open_response.json()  \n",
    "        open_pulls_count = pulls_open_data.get('total_count', 0)  \n",
    "    else:  \n",
    "        print(f\"Failed to fetch open pull requests for {repo_url}: {pulls_open_response.status_code}\")  \n",
    "        open_pulls_count = 0  \n",
    "      \n",
    "    # Fetch closed pull requests count  \n",
    "    pulls_closed_response = requests.get(pulls_closed_url, headers=headers)  \n",
    "    if pulls_closed_response.status_code == 200:  \n",
    "        pulls_closed_data = pulls_closed_response.json()  \n",
    "        closed_pulls_count = pulls_closed_data.get('total_count', 0)  \n",
    "    else:  \n",
    "        print(f\"Failed to fetch closed pull requests for {repo_url}: {pulls_closed_response.status_code}\")  \n",
    "        closed_pulls_count = 0  \n",
    "      \n",
    "    # Fetch commit count  \n",
    "    commits_response = requests.get(commits_url, headers=headers)  \n",
    "    if commits_response.status_code == 200:  \n",
    "        commits_data = commits_response.json()  \n",
    "        commit_count = len(commits_data)  \n",
    "        # Estimate total commit count by checking the last page  \n",
    "        last_page = commits_response.links.get('last', {}).get('url')  \n",
    "        if last_page:  \n",
    "            last_page_response = requests.get(last_page, headers=headers)  \n",
    "            if last_page_response.status_code == 200:  \n",
    "                last_page_data = last_page_response.json()  \n",
    "                commit_count += (len(last_page_data) - 1) * 30  # Assuming 30 commits per page  \n",
    "        else:  \n",
    "            commit_count = len(commits_data)  \n",
    "    else:  \n",
    "        print(f\"Failed to fetch commits for {repo_url}: {commits_response.status_code}\")  \n",
    "        commit_count = 0  \n",
    "      \n",
    "    # Extract basic metrics  \n",
    "    metrics = {  \n",
    "        'url': repo_url,  \n",
    "        'open_issues_count': open_issues_count,  \n",
    "        'closed_issues_count': closed_issues_count,  \n",
    "        'open_pulls_count': open_pulls_count,  \n",
    "        'closed_pulls_count': closed_pulls_count,  \n",
    "        'commit_count': commit_count  \n",
    "    }  \n",
    "      \n",
    "    return metrics  \n",
    "  \n",
    "# Function to handle rate limiting  \n",
    "def handle_rate_limiting(headers):  \n",
    "    rate_limit_url = 'https://api.github.com/rate_limit'  \n",
    "    response = requests.get(rate_limit_url, headers=headers)  \n",
    "    if response.status_code == 200:  \n",
    "        rate_limit_data = response.json()  \n",
    "        remaining = rate_limit_data['rate']['remaining']  \n",
    "        reset_time = rate_limit_data['rate']['reset']  \n",
    "        if remaining == 0:  \n",
    "            reset_timestamp = datetime.fromtimestamp(reset_time)  \n",
    "            current_time = datetime.now()  \n",
    "            wait_time = (reset_timestamp - current_time).total_seconds() + 5  # Add 5 seconds buffer  \n",
    "            print(f\"Rate limit reached. Waiting for {wait_time} seconds until reset.\")  \n",
    "            time.sleep(wait_time)  \n",
    "    else:  \n",
    "        print(f\"Failed to fetch rate limit information: {response.status_code}\")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to mirror the DataFrame  \n",
    "def mirror_dataframe(df):  \n",
    "    mirrored_df = df.copy()  \n",
    "    mirrored_df['project_a'], mirrored_df['project_b'] = df['project_b'], df['project_a']  \n",
    "    mirrored_df['weight_a'], mirrored_df['weight_b'] = df['weight_b'], df['weight_a']  \n",
    "    return mirrored_df  \n",
    " \n",
    "# Load the datasets  \n",
    "hf_test = pd.read_csv('raw_dataset/hf/test.csv')  \n",
    "pond_test = pd.read_csv('raw_dataset/pond/test.csv')  \n",
    "oso_train = pd.read_csv('raw_dataset/OSO/dataset.csv')  \n",
    "hf_train = pd.read_csv('raw_dataset/hf/dataset.csv')  \n",
    "pond_train = pd.read_csv('raw_dataset/pond/dataset.csv')  \n",
    "  \n",
    "# Load the metrics DataFrame  \n",
    "metrics_path = 'metrics_with_summary.csv'  \n",
    "metrics_df = pd.read_csv(metrics_path)  \n",
    "  \n",
    "# Ensure URLs in metrics are unique and can be used for joining  \n",
    "metrics_df = metrics_df.drop_duplicates(subset='url')  \n",
    "  \n",
    "# Rename columns in metrics to add suffixes for project_a and project_b  \n",
    "metrics_a = metrics_df.rename(columns=lambda col: f\"{col}_project_a\" if col != 'url' else 'url')  \n",
    "metrics_b = metrics_df.rename(columns=lambda col: f\"{col}_project_b\" if col != 'url' else 'url')  \n",
    "  \n",
    "# Function to enrich the dataset with metrics  \n",
    "def enrich_dataset(df, metrics_a, metrics_b):  \n",
    "    # Merge metrics data for project_a  \n",
    "    enriched_df = df.merge(metrics_a, how='left', left_on='project_a', right_on='url')  \n",
    "    enriched_df.drop(columns='url', inplace=True)  # Drop extra 'url' column from merge  \n",
    "      \n",
    "    # Merge metrics data for project_b  \n",
    "    enriched_df = enriched_df.merge(metrics_b, how='left', left_on='project_b', right_on='url')  \n",
    "    enriched_df.drop(columns='url', inplace=True)  # Drop extra 'url' column from merge  \n",
    "      \n",
    "    return enriched_df  \n",
    "  \n",
    "GitHub token  \n",
    "GITHUB_TOKEN = 'github token'  \n",
    "  \n",
    "# Headers for GitHub API requests  \n",
    "headers = {  \n",
    "    'Authorization': f'token {GITHUB_TOKEN}',  \n",
    "    'Accept': 'application/vnd.github.v3+json'  \n",
    "}  \n",
    "  \n",
    "# Process each dataset  \n",
    "datasets = {  \n",
    "    'hf_test': hf_test,  \n",
    "    'pond_test': pond_test,  \n",
    "    'aug_train': oso_train,  #OSO data\n",
    "    'hf_train': hf_train,  \n",
    "    'pond_train': pond_train  \n",
    "}  \n",
    "  \n",
    "enriched_datasets = {}  \n",
    "  \n",
    "for name, df in datasets.items():  \n",
    "    # Create the mirrored DataFrame  \n",
    "    mirrored_df = mirror_dataframe(df)  \n",
    "      \n",
    "    # Concatenate the original and mirrored DataFrames  \n",
    "    combined_df = pd.concat([df, mirrored_df], ignore_index=True)  \n",
    "      \n",
    "    # Extract unique URLs for project_a and project_b  \n",
    "    unique_urls = combined_df['project_a'].tolist() + combined_df['project_b'].tolist()  \n",
    "    unique_urls = list(set(unique_urls))  \n",
    "      \n",
    "    # Fetch metrics for each unique URL  \n",
    "    all_metrics = []  \n",
    "    for url in unique_urls:  \n",
    "        metrics = get_github_metrics(url, headers)  \n",
    "        if metrics:  \n",
    "            all_metrics.append(metrics)  \n",
    "        handle_rate_limiting(headers)  # Check and handle rate limiting  \n",
    "      \n",
    "    # Convert the metrics to a DataFrame  \n",
    "    metrics_df = pd.DataFrame(all_metrics)  \n",
    "      \n",
    "    # Ensure URLs in metrics are unique and can be used for joining  \n",
    "    metrics_df = metrics_df.drop_duplicates(subset='url')  \n",
    "      \n",
    "    # Rename columns in metrics to add suffixes for project_a and project_b  \n",
    "    metrics_a = metrics_df.rename(columns=lambda col: f\"{col}_project_a\" if col != 'url' else 'url')  \n",
    "    metrics_b = metrics_df.rename(columns=lambda col: f\"{col}_project_b\" if col != 'url' else 'url')  \n",
    "      \n",
    "    # Enrich the combined DataFrame with metrics  \n",
    "    enriched_df = enrich_dataset(combined_df, metrics_a, metrics_b)  \n",
    "      \n",
    "    # Save the enriched DataFrame to a CSV file  \n",
    "    enriched_df.to_csv(f'enriched_{name}.csv', index=False)  \n",
    "      \n",
    "    # Store the enriched DataFrame for verification  \n",
    "    enriched_datasets[name] = enriched_df  \n",
    "      \n",
    "    print(f\"Processed and saved enriched DataFrame for {name} to enriched_{name}.csv\")  \n",
    "  \n",
    "# Optionally, print the head of each enriched DataFrame  \n",
    "for name, enriched_df in enriched_datasets.items():  \n",
    "    print(f\"Enriched DataFrame for {name} head:\")  \n",
    "    print(enriched_df.head())  \n",
    "    print(\"\\n\")  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_dlib",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\Anaconda3\\envs\\env_dlib\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "from sklearn.model_selection import train_test_split  \n",
    "from sklearn.preprocessing import LabelEncoder  \n",
    "import xgboost as xgb  \n",
    "from sklearn.metrics import mean_squared_error, r2_score  \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load training and test data (from pond and huggingface) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract repository name from URL  \n",
    "def extract_repo_name(url):  \n",
    "    try:  \n",
    "        return url.split('/')[-2] + '/' + url.split('/')[-1]  \n",
    "    except:  \n",
    "        return None  \n",
    "  \n",
    "\n",
    "# Load the test dataset  \n",
    "test_data_path = r'enriched_dataset_with_summary/hf_test_enriched_with_summary.csv'  \n",
    "test_data = pd.read_csv(test_data_path)  \n",
    "\n",
    "# Load additional training data\n",
    "additional_data_path = r'enriched_dataset_with_summary/pond_train_enriched_with_summary.csv'      \n",
    "train_add = pd.read_csv(additional_data_path)      \n",
    "  \n",
    "# Load target training data     \n",
    "train_data_path = r'enriched_dataset_with_summary/hf_train_enriched_with_summary.csv'      \n",
    "train_hf = pd.read_csv(train_data_path)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minority class resampling\n",
    "I resampling data for minority projects to balance the data, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Distribution of project_a:\n",
      "project_a\n",
      "https://github.com/motdotla/dotenv           66\n",
      "https://github.com/sindresorhus/type-fest    66\n",
      "https://github.com/zloirock/core-js          65\n",
      "https://github.com/postcss/postcss           64\n",
      "https://github.com/immerjs/immer             64\n",
      "                                             ..\n",
      "https://github.com/quic-go/quic-go            9\n",
      "https://github.com/ethereum/solc-js           8\n",
      "https://github.com/erigontech/erigon          8\n",
      "https://github.com/grandinetech/grandine      8\n",
      "https://github.com/ethereum/web3.py           1\n",
      "Name: count, Length: 117, dtype: int64\n",
      "\n",
      "Balanced Distribution of project_a:\n",
      "project_a\n",
      "https://github.com/formatjs/formatjs                     675\n",
      "https://github.com/wooorm/markdown-table                 675\n",
      "https://github.com/eth-infinitism/account-abstraction    675\n",
      "https://github.com/xtuc/webassemblyjs                    675\n",
      "https://github.com/vyperlang/vyper                       675\n",
      "                                                        ... \n",
      "https://github.com/pion/webrtc                           675\n",
      "https://github.com/numpy/numpy                           675\n",
      "https://github.com/postcss/postcss                       675\n",
      "https://github.com/sindresorhus/type-fest                 66\n",
      "https://github.com/motdotla/dotenv                        66\n",
      "Name: count, Length: 117, dtype: int64\n",
      "\n",
      "Total number of rows in oversampled train_hf: 77757\n",
      "Target number of rows: 83832\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Calculate the target number of rows for train_hf\n",
    "target_rows = 2 * len(train_add)\n",
    "\n",
    "# Check the distribution of the 'project_a' column\n",
    "project_a_counts = train_hf['project_a'].value_counts()\n",
    "print(\"Original Distribution of project_a:\")\n",
    "print(project_a_counts)\n",
    "\n",
    "# Identify the maximum count\n",
    "max_count = project_a_counts.max()\n",
    "\n",
    "# Calculate the total number of rows needed for each class to reach the target\n",
    "total_rows_needed = target_rows - len(train_hf)\n",
    "\n",
    "# Calculate the required oversampling factor for each class\n",
    "oversampling_factors = {}\n",
    "for project, count in project_a_counts.items():\n",
    "    if count < max_count:\n",
    "        oversampling_factors[project] = (total_rows_needed / len(project_a_counts)) / count\n",
    "    else:\n",
    "        oversampling_factors[project] = 1\n",
    "\n",
    "# Create a list to hold the oversampled dataframes\n",
    "oversampled_dfs = []\n",
    "\n",
    "# Iterate over each unique value in the 'project_a' column\n",
    "for project, count in project_a_counts.items():\n",
    "    # Resample the dataframe for the current project\n",
    "    df_project = train_hf[train_hf['project_a'] == project]\n",
    "    n_samples = int(count * oversampling_factors[project])\n",
    "    if n_samples < count:\n",
    "        n_samples = count  # Ensure at least the original count\n",
    "    df_project_oversampled = resample(df_project, \n",
    "                                      replace=True,     # Sample with replacement\n",
    "                                      n_samples=n_samples,    # To match the required factor\n",
    "                                      random_state=42) # Random state for reproducibility\n",
    "    oversampled_dfs.append(df_project_oversampled)\n",
    "\n",
    "# Concatenate all the oversampled dataframes\n",
    "train_hf_oversampled = pd.concat(oversampled_dfs)\n",
    "\n",
    "# Verify the new distribution\n",
    "new_project_a_counts = train_hf_oversampled['project_a'].value_counts()\n",
    "print(\"\\nBalanced Distribution of project_a:\")\n",
    "print(new_project_a_counts)\n",
    "\n",
    "# Verify the total number of rows\n",
    "print(f\"\\nTotal number of rows in oversampled train_hf: {len(train_hf_oversampled)}\")\n",
    "print(f\"Target number of rows: {target_rows}\")\n",
    "\n",
    "# Combine the oversampled dataset_hf with train_add  \n",
    "dataset_aug = pd.concat([train_add, train_hf_oversampled], ignore_index=True)      \n",
    "dataset_aug.reset_index(drop=True, inplace=True)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "pd.set_option('display.width', 1000)        # Set the display width\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extract repository names for both datasets  \n",
    "dataset_aug['repo_name_a'] = dataset_aug['project_a'].apply(extract_repo_name)  \n",
    "dataset_aug['repo_name_b'] = dataset_aug['project_b'].apply(extract_repo_name)  \n",
    "test_data['repo_name_a'] = test_data['project_a'].apply(extract_repo_name)  \n",
    "test_data['repo_name_b'] = test_data['project_b'].apply(extract_repo_name)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use logarithmic function to handle skewed data\n",
    "Numerical metrics data in github repos can be highly skewed. e.g. some repos can have 0 used_by count,\n",
    "but the other have 2M+ used by count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply logarithmic transformations for the new columns\n",
    "for dataset in [dataset_aug, test_data]:\n",
    "    for project in ['a', 'b']:\n",
    "        for column in ['open_issues_count', 'closed_issues_count', 'open_prs_count', 'closed_prs_count', 'used_by', 'age_days',\n",
    "                       'size', 'subscribers_count'\n",
    "                       \n",
    "        ]:\n",
    "            col_name = f'{column}_project_{project}'\n",
    "            log_col_name = f'log_{column}_project_{project}'\n",
    "            if col_name in dataset.columns and log_col_name not in dataset.columns:\n",
    "                dataset[log_col_name] = np.log1p(dataset[col_name])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use TF-IDF to help the models read Readme summary and its importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=10)  # You can adjust max_features as needed\n",
    "\n",
    "# Fit and transform the summary column for project_a in the training dataset\n",
    "tfidf_a = tfidf_vectorizer.fit_transform(dataset_aug['summary_project_a'])\n",
    "tfidf_a_df = pd.DataFrame(tfidf_a.toarray(), columns=[f'tfidf_a_{i}' for i in range(tfidf_a.shape[1])])\n",
    "dataset_aug = pd.concat([dataset_aug, tfidf_a_df], axis=1)\n",
    "\n",
    "# Transform the summary column for project_a in the test dataset\n",
    "tfidf_a_test = tfidf_vectorizer.transform(test_data['summary_project_a'])\n",
    "tfidf_a_test_df = pd.DataFrame(tfidf_a_test.toarray(), columns=[f'tfidf_a_{i}' for i in range(tfidf_a_test.shape[1])])\n",
    "test_data = pd.concat([test_data, tfidf_a_test_df], axis=1)\n",
    "\n",
    "# Fit and transform the summary column for project_b in the training dataset\n",
    "tfidf_b = tfidf_vectorizer.fit_transform(dataset_aug['summary_project_b'])\n",
    "tfidf_b_df = pd.DataFrame(tfidf_b.toarray(), columns=[f'tfidf_b_{i}' for i in range(tfidf_b.shape[1])])\n",
    "dataset_aug = pd.concat([dataset_aug, tfidf_b_df], axis=1)\n",
    "\n",
    "# Transform the summary column for project_b in the test dataset\n",
    "tfidf_b_test = tfidf_vectorizer.transform(test_data['summary_project_b'])\n",
    "tfidf_b_test_df = pd.DataFrame(tfidf_b_test.toarray(), columns=[f'tfidf_b_{i}' for i in range(tfidf_b_test.shape[1])])\n",
    "test_data = pd.concat([test_data, tfidf_b_test_df], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Select features and target variable  \n",
    "features = [  \n",
    "    'total_amount_usd',\n",
    "    # Feature A  \n",
    "    'is_private_project_a',\n",
    "    'has_homepage_project_a',  \n",
    "    'log_size_project_a', \n",
    "    'stars_project_a', \n",
    "    'watchers_project_a',  \n",
    "    'has_projects_project_a', \n",
    "    'has_pages_project_a', \n",
    "    'has_wiki_project_a',  \n",
    "    'has_discussions_project_a', \n",
    "    'forks_project_a', \n",
    "    'is_archived_project_a',  \n",
    "    'is_disabled_project_a', \n",
    "    'open_issues_project_a', \n",
    "    'subscribers_count_project_a',  \n",
    "    'age_days_project_a', \n",
    "    'days_since_update_project_a', \n",
    "    'stars_ratio_project_a',  \n",
    "    'watchers_ratio_project_a', \n",
    "    'forks_ratio_project_a', \n",
    "    'size_ratio_project_a',  \n",
    "    'log_stars_project_a',  \n",
    "    'log_watchers_project_a', \n",
    "    'log_forks_project_a', \n",
    "    'log_commit_count_project_a',  \n",
    "    # 'log_stars_b_project_a', \n",
    "    # 'log_watchers_b_project_a', \n",
    "    # 'log_forks_b_project_a',  \n",
    "    # 'log_commit_count_b_project_a',\n",
    "    # Feature B\n",
    "    'is_private_project_b', \n",
    "    'has_homepage_project_b',  \n",
    "    'log_size_project_b', \n",
    "    'stars_project_b', \n",
    "    'watchers_project_b',  \n",
    "    'has_projects_project_b', \n",
    "    'has_pages_project_b', \n",
    "    'has_wiki_project_b',  \n",
    "    'has_discussions_project_b', \n",
    "    'forks_project_b', \n",
    "    'is_archived_project_b',  \n",
    "    'is_disabled_project_b', \n",
    "    'open_issues_project_b', \n",
    "    'subscribers_count_project_b',  \n",
    "    'age_days_project_b', \n",
    "    'days_since_update_project_b', \n",
    "    'stars_ratio_project_b',  \n",
    "    'watchers_ratio_project_b', \n",
    "    'forks_ratio_project_b', \n",
    "    'size_ratio_project_b',  \n",
    "    'log_stars_project_b',  \n",
    "    'log_watchers_project_b', \n",
    "    'log_forks_project_b', \n",
    "    'log_commit_count_project_b',  \n",
    "    # 'log_stars_b_project_b', \n",
    "    # 'log_watchers_b_project_b', \n",
    "    # 'log_forks_b_project_b',  \n",
    "    # 'log_commit_count_b_project_b',  \n",
    "    'repo_name_b','repo_name_a',\n",
    "    # Additional Logarithmic Features\n",
    "    'log_open_issues_count_project_a', 'log_closed_issues_count_project_a', 'log_open_prs_count_project_a', 'log_closed_prs_count_project_a', 'log_used_by_project_a', 'log_age_days_project_a',\n",
    "    'log_open_issues_count_project_b', 'log_closed_issues_count_project_b', 'log_open_prs_count_project_b', 'log_closed_prs_count_project_b', 'log_used_by_project_b', 'log_age_days_project_b'\n",
    "]  \n",
    "\n",
    "# Get the list of new TF-IDF feature names\n",
    "tfidf_feature_names_a = [f'tfidf_a_{i}' for i in range(tfidf_a.shape[1])]\n",
    "tfidf_feature_names_b = [f'tfidf_b_{i}' for i in range(tfidf_b.shape[1])]\n",
    "\n",
    "# Update features list with new TF-IDF features\n",
    "features.extend(tfidf_feature_names_a + tfidf_feature_names_b)\n",
    "  \n",
    "log_features = [  \n",
    "    'log_subscribers_count_project_a','log_subscribers_count_project_b',\n",
    "    'log_size_project_a', 'log_stars_project_a', 'log_watchers_project_a', 'log_forks_project_a', 'log_commit_count_project_a',  \n",
    "    'log_stars_b_project_a', 'log_watchers_b_project_a', 'log_forks_b_project_a', 'log_commit_count_b_project_a',  \n",
    "    'log_size_project_b', 'log_stars_project_b', 'log_watchers_project_b', 'log_forks_project_b', 'log_commit_count_project_b',  \n",
    "    'log_stars_b_project_b', 'log_watchers_b_project_b', 'log_forks_b_project_b', 'log_commit_count_b_project_b',\n",
    "    'log_open_issues_count_project_a', 'log_closed_issues_count_project_a', 'log_open_prs_count_project_a', 'log_closed_prs_count_project_a', 'log_used_by_project_a', 'log_age_days_project_a',\n",
    "    'log_open_issues_count_project_b', 'log_closed_issues_count_project_b', 'log_open_prs_count_project_b', 'log_closed_prs_count_project_b', 'log_used_by_project_b', 'log_age_days_project_b'\n",
    "]  \n",
    "\n",
    "plain_features = [  \n",
    "    'size_project_a', 'stars_project_a', 'watchers_project_a', 'forks_project_a', 'commit_count_project_a',  \n",
    "    'stars_b_project_a', 'watchers_b_project_a', 'forks_b_project_a', 'commit_count_b_project_a',  \n",
    "    'size_project_b', 'stars_project_b', 'watchers_project_b', 'forks_project_b', 'commit_count_project_b',  \n",
    "    'stars_b_project_b', 'watchers_b_project_b', 'forks_b_project_b', 'commit_count_b_project_b',\n",
    "    'subscribers_count_project_b', 'subscribers_count_project_a',\n",
    "    'open_issues_count_project_a', 'closed_issues_count_project_a', 'open_prs_count_project_a', 'closed_prs_count_project_a', 'used_by_project_a', 'age_days_project_a',\n",
    "    'open_issues_count_project_b', 'closed_issues_count_project_b', 'open_prs_count_project_b', 'closed_prs_count_project_b', 'used_by_project_b', 'age_days_project_b'\n",
    "]  \n",
    "\n",
    "# Remove plain features if log features are present  \n",
    "features = [col for col in features if col not in plain_features or col in log_features]  \n",
    "\n",
    "# Target variable  \n",
    "target = 'weight_a'  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorical variable encoding\n",
    "Quarter should be handled properly as its a time-based data. But for simplicity, i use quarter column as categorical variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Encode categorical variables  \n",
    "label_encoders = {}  \n",
    "for col in ['repo_name_a', 'repo_name_b','funder','quarter']:  \n",
    "    if col in dataset_aug.columns:  \n",
    "        le = LabelEncoder()  \n",
    "        dataset_aug[col] = le.fit_transform(dataset_aug[col])  \n",
    "        label_encoders[col] = le  \n",
    "  \n",
    "    if col in test_data.columns:  \n",
    "        # Use transform with error handling for unseen labels  \n",
    "        try:  \n",
    "            test_data[col] = le.transform(test_data[col])  \n",
    "        except ValueError:  \n",
    "            # Assign a default value for unseen labels  \n",
    "            test_data[col] = -1  \n",
    "\n",
    "# Convert boolean columns to integer  \n",
    "boolean_cols = [  \n",
    "    'is_private_project_a', 'has_homepage_project_a', 'has_projects_project_a',  \n",
    "    'has_pages_project_a', 'has_wiki_project_a', 'has_discussions_project_a',  \n",
    "    'is_archived_project_a', 'is_disabled_project_a',  \n",
    "    'is_private_project_b', 'has_homepage_project_b', 'has_projects_project_b',  \n",
    "    'has_pages_project_b', 'has_wiki_project_b', 'has_discussions_project_b',  \n",
    "    'is_archived_project_b', 'is_disabled_project_b'  \n",
    "]  \n",
    "\n",
    "for col in boolean_cols:  \n",
    "    if col in dataset_aug.columns:  \n",
    "        dataset_aug[col] = dataset_aug[col].astype(int)  \n",
    "    if col in test_data.columns:  \n",
    "        test_data[col] = test_data[col].astype(int)  \n",
    "\n",
    "# Filter out features that are not present in the dataset  \n",
    "features = [col for col in features if col in dataset_aug.columns and col in test_data.columns]  \n",
    "\n",
    "# Select features and target  \n",
    "X = dataset_aug[features]  \n",
    "y = dataset_aug[target]  \n",
    "\n",
    "# Split the data into training and testing sets  \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize XGBoost models\n",
    "Ive running down a hyperparameter grid search to find the best parameters, and the result is used here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.012649668472400157\n",
      "R^2 Score: 0.8997269884330764\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize the XGBoost regressor  \n",
    "xgb_regressor = xgb.XGBRegressor(  \n",
    "    objective='reg:squarederror',  \n",
    "    n_estimators=750,  \n",
    "    learning_rate=0.05,  \n",
    "    max_depth=8,  \n",
    "    subsample=1,  \n",
    "    colsample_bytree=0.1,\n",
    "    random_state=42  \n",
    ")  \n",
    "\n",
    "# Train the model  \n",
    "xgb_regressor.fit(X_train, y_train)  \n",
    "\n",
    "# Make predictions  \n",
    "y_pred = xgb_regressor.predict(X_test)  \n",
    "\n",
    "# Ensure predictions are within the range [0, 1]  \n",
    "y_pred = y_pred.clip(min=0, max=1)  \n",
    "\n",
    "# Calculate evaluation metrics  \n",
    "mse = mean_squared_error(y_test, y_pred)  \n",
    "r2 = r2_score(y_test, y_pred)  \n",
    "\n",
    "print(f\"Mean Squared Error: {mse}\")  \n",
    "print(f\"R^2 Score: {r2}\")  \n",
    "\n",
    "test_features = [col for col in features if col in test_data.columns]  \n",
    "\n",
    "# Select features for test data  \n",
    "X_test_final = test_data[test_features]  \n",
    "\n",
    "# Make predictions on the test dataset  \n",
    "y_pred_test = xgb_regressor.predict(X_test_final)  \n",
    "\n",
    "# Ensure predictions are within the range [0, 1]  \n",
    "y_pred_test = y_pred_test.clip(min=0, max=1)  \n",
    "\n",
    "# Calculate weight_b as 1 - weight_a  \n",
    "y_pred_test_b = 1 - y_pred_test  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correct transitivity in prediction (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrected predictions saved to dumphf10-pondtrain_x20_750_005_8_1_wA_wB_transitivity.csv\n"
     ]
    }
   ],
   "source": [
    "predictions_df = pd.DataFrame({  \n",
    "    'id': test_data['id'],  \n",
    "    'weight_a': y_pred_test,  \n",
    "    'weight_b': y_pred_test_b  \n",
    "})  \n",
    "  \n",
    "# Function to correct transitivity violations  \n",
    "def correct_transitivity(predictions):  \n",
    "    # Sort by weight_a  \n",
    "    predictions = predictions.sort_values(by='weight_a').reset_index(drop=True)  \n",
    "      \n",
    "    # Iterate through the DataFrame and correct violations  \n",
    "    for i in range(1, len(predictions)):  \n",
    "        if predictions.iloc[i]['weight_a'] < predictions.iloc[i-1]['weight_a']:  \n",
    "            predictions.iloc[i]['weight_a'] = predictions.iloc[i-1]['weight_a']  \n",
    "            predictions.iloc[i]['weight_b'] = 1 - predictions.iloc[i]['weight_a']  # Adjust weight_b accordingly  \n",
    "  \n",
    "    return predictions  \n",
    "  \n",
    "# Apply the transitivity correction  \n",
    "corrected_predictions_df = correct_transitivity(predictions_df) \n",
    "submission_df= corrected_predictions_df[['id','weight_a']] \n",
    "corrected_predictions_df = predictions_df\n",
    "  \n",
    "# Save the corrected predictions to a CSV file  \n",
    "save_path = 'dump'  \n",
    "filename = 'hf10-pondtrain_x20_750_005_8_1_wA_wB_transitivity.csv'  \n",
    "corrected_predictions_df.to_csv(save_path + filename, index=False)  \n",
    "  \n",
    "print(\"Corrected predictions saved to\", save_path + filename)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### native prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wA wB predictions saved to dump/predictions_df.csv\n"
     ]
    }
   ],
   "source": [
    "predictions_df = pd.DataFrame({  \n",
    "    'id': test_data['id'],  \n",
    "    'weight_a': y_pred_test,  \n",
    "    'weight_b': y_pred_test_b  \n",
    "})  \n",
    " \n",
    "# Save native predictions to a CSV file  \n",
    "save_path = 'dump/'  \n",
    "filename = 'predictions_df.csv'  \n",
    "predictions_df.to_csv(save_path + filename, index=False)  \n",
    "  \n",
    "print(\"wA wB predictions saved to\", save_path + filename)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     id  weight_a  weight_b\n",
      "0     1  0.067631  0.932369\n",
      "1     8  0.444064  0.555936\n",
      "2    13  0.324559  0.675441\n",
      "3    15  0.358210  0.641790\n",
      "4    18  0.976285  0.023715\n",
      "5    23  0.877279  0.122721\n",
      "6    26  0.422457  0.577543\n",
      "7    27  0.434655  0.565345\n",
      "8    30  0.914351  0.085649\n",
      "9    31  0.710026  0.289974\n",
      "10   32  0.724098  0.275902\n",
      "11   33  0.608229  0.391771\n",
      "12   34  0.665071  0.334929\n",
      "13   44  0.820792  0.179208\n",
      "14   45  0.622694  0.377306\n",
      "15   46  0.801497  0.198503\n",
      "16   49  0.714008  0.285992\n",
      "17   52  0.336273  0.663727\n",
      "18   53  0.527621  0.472379\n",
      "19   57  0.701846  0.298154\n",
      "20   59  0.419123  0.580877\n",
      "21   60  0.492780  0.507220\n",
      "22   64  0.183109  0.816891\n",
      "23   68  0.003921  0.996080\n",
      "24   70  0.082931  0.917069\n",
      "25   71  0.086752  0.913248\n",
      "26   74  0.003668  0.996332\n",
      "27   77  0.089354  0.910646\n",
      "28   81  0.123605  0.876395\n",
      "29   88  0.000000  1.000000\n",
      "30   89  0.122838  0.877162\n",
      "31   94  0.024976  0.975024\n",
      "32   97  0.233977  0.766023\n",
      "33  103  0.069730  0.930270\n",
      "34  105  0.931194  0.068806\n",
      "35  109  0.654555  0.345445\n",
      "36  110  0.789169  0.210831\n",
      "37  112  0.730002  0.269998\n",
      "38  121  0.717394  0.282606\n",
      "39  125  0.547278  0.452722\n"
     ]
    }
   ],
   "source": [
    "print(predictions_df.head(40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed CSV saved to: submission/XGB_BART_metrics_and_summary_result.csv\n"
     ]
    }
   ],
   "source": [
    " # file_path = r'D:\\0000Pond\\funding comp\\huggingface-funding-comp\\x10_1000_005_wA_wB_transitivity.csv'  # Update with the cleaned file path  \n",
    "df = pd.read_csv(save_path + filename)      \n",
    "\n",
    "# Normalize the weights to ensure weight_a + weight_b = 1  \n",
    "df['total_weight'] = df['weight_a'] + df['weight_b']  \n",
    "df['weight_a'] = df['weight_a'] / df['total_weight']  \n",
    "df['weight_b'] = df['weight_b'] / df['total_weight']  \n",
    "\n",
    "# Round the weights to 11 decimal points  \n",
    "df['weight_a'] = df['weight_a'].round(11)  \n",
    "df['weight_b'] = df['weight_b'].round(11)  \n",
    "\n",
    "# Rename the 'weight_a' column to 'pred'\n",
    "df.rename(columns={'weight_a': 'pred'}, inplace=True)\n",
    "\n",
    "# Drop the total_weight and weight_b columns as they're no longer needed  \n",
    "df.drop(columns=['total_weight', 'weight_b'], inplace=True)  \n",
    "\n",
    "# Sort the DataFrame by 'id'  \n",
    "df.sort_values(by='id', inplace=True)  \n",
    "\n",
    "# Save the modified DataFrame to a new CSV file  \n",
    "output_path = r'submission/XGB_BART_metrics_and_summary_result.csv'  \n",
    "df.to_csv(output_path, index=False)  \n",
    "\n",
    "print(\"Processed CSV saved to:\", output_path)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.067631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>0.444064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13</td>\n",
       "      <td>0.324559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15</td>\n",
       "      <td>0.358210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18</td>\n",
       "      <td>0.976285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1018</th>\n",
       "      <td>3400</td>\n",
       "      <td>0.380968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1019</th>\n",
       "      <td>3403</td>\n",
       "      <td>0.554686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1020</th>\n",
       "      <td>3405</td>\n",
       "      <td>0.411174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1021</th>\n",
       "      <td>3406</td>\n",
       "      <td>0.417776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1022</th>\n",
       "      <td>3409</td>\n",
       "      <td>0.554109</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1023 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id      pred\n",
       "0        1  0.067631\n",
       "1        8  0.444064\n",
       "2       13  0.324559\n",
       "3       15  0.358210\n",
       "4       18  0.976285\n",
       "...    ...       ...\n",
       "1018  3400  0.380968\n",
       "1019  3403  0.554686\n",
       "1020  3405  0.411174\n",
       "1021  3406  0.417776\n",
       "1022  3409  0.554109\n",
       "\n",
       "[1023 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Transitivity Violations\n",
    "This code is from FaezehShakouri https://github.com/FaezehShakouri/deepfunding/blob/main/model/src/transivity_check.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 transitivity violations\n",
      "\n",
      "Detailed violations:\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import itertools\n",
    "\n",
    "# Load predictions and original data\n",
    "predictions_df = pd.read_csv(output_path)\n",
    "test_data = pd.read_csv(\"raw_dataset/hf/test.csv\")\n",
    "\n",
    "# Create a dictionary mapping id to prediction\n",
    "id_to_pred = dict(zip(predictions_df['id'], predictions_df['pred']))\n",
    "\n",
    "# Get pairs from original data\n",
    "pairs = []\n",
    "for _, row in test_data.iterrows():\n",
    "    if pd.notna(row['project_a']) and pd.notna(row['project_b']):\n",
    "        pairs.append((row['id'], row['project_a'], row['project_b']))\n",
    "\n",
    "# Build graph of connected pairs\n",
    "connected_pairs = set()\n",
    "for id1, proj_a, proj_b in pairs:\n",
    "    connected_pairs.add((id1, proj_a))\n",
    "    connected_pairs.add((id1, proj_b))\n",
    "\n",
    "# Function to check transitivity for a triplet\n",
    "def check_transitivity(id1, id2, id3):\n",
    "    pred1 = id_to_pred[id1]\n",
    "    pred2 = id_to_pred[id2]\n",
    "    pred3 = id_to_pred[id3]\n",
    "    \n",
    "    # Check if pred1 < pred2 and pred2 < pred3\n",
    "    if pred1 < pred2 and pred2 < pred3:\n",
    "        # If true, pred1 should be < pred3\n",
    "        if not pred1 < pred3:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "# Find all inconsistencies among connected triplets\n",
    "inconsistencies = []\n",
    "for id1, id2, id3 in itertools.combinations(id_to_pred.keys(), 3):\n",
    "    # Check if these IDs form connected pairs in original data\n",
    "    if ((id1, id2) in connected_pairs and \n",
    "        (id2, id3) in connected_pairs and\n",
    "        (id1, id3) in connected_pairs):\n",
    "        if not check_transitivity(id1, id2, id3):\n",
    "            inconsistencies.append((id1, id2, id3))\n",
    "\n",
    "# Print results\n",
    "print(f\"Found {len(inconsistencies)} transitivity violations\")\n",
    "print(\"\\nDetailed violations:\")\n",
    "for id1, id2, id3 in inconsistencies:\n",
    "    print(f\"\\nViolation between IDs {id1}, {id2}, {id3}:\")\n",
    "    print(f\"Pred({id1}) = {id_to_pred[id1]:.4f}\")\n",
    "    print(f\"Pred({id2}) = {id_to_pred[id2]:.4f}\")\n",
    "    print(f\"Pred({id3}) = {id_to_pred[id3]:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
